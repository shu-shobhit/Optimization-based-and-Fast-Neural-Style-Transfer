{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import v2\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modules in vgg19\n",
    "- **0**: conv1_1 - Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **1**: ReLU(inplace=True)\n",
    "- **2**: conv1_2 - Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **3**: ReLU(inplace=True)\n",
    "- **4**: pool1 - MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "- **5**: conv2_1 - Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **6**: ReLU(inplace=True)\n",
    "- **7**: conv2_2 - Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **8**: ReLU(inplace=True)\n",
    "- **9**: pool2 - MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "- **10**: conv3_1 - Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **11**: ReLU(inplace=True)\n",
    "- **12**: conv3_2 - Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **13**: ReLU(inplace=True)\n",
    "- **14**: conv3_3 - Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **15**: ReLU(inplace=True)\n",
    "- **16**: conv3_4 - Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **17**: ReLU(inplace=True)\n",
    "- **18**: pool3 - MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "- **19**: conv4_1 - Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **20**: ReLU(inplace=True)\n",
    "- **21**: conv4_2 - Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **22**: ReLU(inplace=True)\n",
    "- **23**: conv4_3 - Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **24**: ReLU(inplace=True)\n",
    "- **25**: conv4_4 - Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **26**: ReLU(inplace=True)\n",
    "- **27**: pool4 - MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "- **28**: conv5_1 - Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **29**: ReLU(inplace=True)\n",
    "- **30**: conv5_2 - Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **31**: ReLU(inplace=True)\n",
    "- **32**: conv5_3 - Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **33**: ReLU(inplace=True)\n",
    "- **34**: conv5_4 - Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **35**: ReLU(inplace=True)\n",
    "- **36**: pool5 - MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self, pool_type: str):\n",
    "        super().__init__()\n",
    "        if pool_type == \"avg\":\n",
    "            self.layer_list = []\n",
    "            self.vgg19 = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features\n",
    "            for layer in self.vgg19:\n",
    "                if isinstance(layer, torch.nn.modules.pooling.MaxPool2d):\n",
    "                    self.layer_list.append(\n",
    "                        nn.AvgPool2d(\n",
    "                            kernel_size=2, stride=2, padding=0, ceil_mode=False\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    self.layer_list.append(layer)\n",
    "            self.final_model = nn.Sequential(*self.layer_list)\n",
    "\n",
    "        else:\n",
    "            self.final_model = models.vgg19().features\n",
    "\n",
    "        for param in self.final_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input, layer_keys):\n",
    "        out = {}\n",
    "        last_layer_key = str(max([int(key) for key in layer_keys]))\n",
    "        for name, layer in self.final_model.named_children():\n",
    "            out[name] = layer(input)\n",
    "            input = out[name]\n",
    "            if name == last_layer_key:\n",
    "                return [out[key] for key in layer_keys]\n",
    "\n",
    "        return [out[key] for key in layer_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralStyleTransfer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        content_image_path,\n",
    "        style_image_path,\n",
    "        content_layer_name,\n",
    "        style_layers_list,\n",
    "        style_layer_weights,\n",
    "        optimizer=\"LBFGS\",\n",
    "        alpha=1,\n",
    "        beta=1000,\n",
    "        lr=1,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        num_steps=500,\n",
    "        resize_value=256,\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self._optim = optimizer\n",
    "        self.content_layer = content_layer_name\n",
    "        self.style_layers_list = style_layers_list\n",
    "        self.style_weights = style_layer_weights\n",
    "        self.num_steps = num_steps\n",
    "        self.content_image = self.load_and_transform(\n",
    "            image_path=content_image_path, resize_value=resize_value\n",
    "        )\n",
    "        self.style_image = self.load_and_transform(\n",
    "            image_path=style_image_path, resize_value=resize_value\n",
    "        )\n",
    "        self.model = VGG19(pool_type=\"avg\").to(device).eval()\n",
    "\n",
    "    def load_and_transform(self, image_path, resize_value):\n",
    "        image = Image.open(image_path)\n",
    "        transformations = v2.Compose(\n",
    "            [\n",
    "                v2.Resize(resize_value),\n",
    "                v2.ToImage(),\n",
    "                v2.ToDtype(torch.float32, scale=True),\n",
    "                v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return transformations(image).unsqueeze(0).to(self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_white_noise(image, device):\n",
    "        batch, channels, h, w = image.shape\n",
    "        random_image = torch.randn(\n",
    "            size=[batch, channels, h, w], requires_grad=True, device=device\n",
    "        )\n",
    "        return random_image\n",
    "\n",
    "    @staticmethod\n",
    "    def tensor_to_image(tensor):\n",
    "        tensor = tensor.clone().detach().squeeze(0).cpu()\n",
    "\n",
    "        tensor = tensor.mul(torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1))\n",
    "        tensor = tensor.add(torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1))\n",
    "        tensor = torch.clamp(tensor, 0, 1)\n",
    "\n",
    "        return v2.ToPILImage()(tensor)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_gram_matrix(input):\n",
    "        batch, channel, height, width = input.shape\n",
    "        reshaped_input = input.view(batch, channel, height * width)\n",
    "        gram_matrix = torch.bmm(reshaped_input, reshaped_input.transpose(1, 2))\n",
    "        gram_matrix = torch.div(gram_matrix, height * width)\n",
    "        return gram_matrix\n",
    "\n",
    "    def visualize_content_rep(self):\n",
    "        print(f\"Using Device: {self.device}\")\n",
    "        random_image = self.generate_white_noise(self.content_image, self.device)\n",
    "\n",
    "        original_img_features = self.model(self.content_image, self.content_layer)[\n",
    "            0\n",
    "        ].detach()\n",
    "\n",
    "        progress_bar = tqdm(range(self.num_steps), desc=\"Optimizing\", unit=\"step\")\n",
    "        if self._optim == \"LBFGS\":\n",
    "            optimizer = optim.LBFGS([random_image], lr=self.lr)\n",
    "            for step in progress_bar:\n",
    "                current_loss = [0.0]\n",
    "\n",
    "                def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    random_img_features = self.model(random_image, self.content_layer)[\n",
    "                        0\n",
    "                    ]\n",
    "                    content_loss = torch.mean(\n",
    "                        (original_img_features - random_img_features) ** 2\n",
    "                    )\n",
    "\n",
    "                    content_loss.backward()\n",
    "                    current_loss[0] = content_loss.item()\n",
    "                    return content_loss\n",
    "\n",
    "                optimizer.step(closure)\n",
    "                progress_bar.set_postfix(loss=current_loss[0])\n",
    "\n",
    "        elif self._optim == \"Adam\":\n",
    "            optimizer = optim.Adam([random_image], lr=self.lr)\n",
    "            for step in progress_bar:\n",
    "                optimizer.zero_grad()\n",
    "                random_img_features = self.model(random_image, self.content_layer)[0]\n",
    "                content_loss = torch.mean(\n",
    "                    (original_img_features - random_img_features) ** 2\n",
    "                )\n",
    "                content_loss.backward()\n",
    "                optimizer.step()\n",
    "                progress_bar.set_postfix(loss=content_loss.item())\n",
    "        generated_image = self.tensor_to_image(random_image)\n",
    "        # plt.imshow(generated_image)\n",
    "        # plt.axis(\"off\")\n",
    "        # plt.show()\n",
    "        return generated_image\n",
    "\n",
    "    def visualize_style_rep(self):\n",
    "        print(f\"Using Device: {self.device}\")\n",
    "        random_image = self.generate_white_noise(self.style_image, self.device)\n",
    "\n",
    "        original_style_features = self.model(self.style_image, self.style_layers_list)\n",
    "        original_gram_matrices = []\n",
    "        for f in original_style_features:\n",
    "            gram_matrix = self.get_gram_matrix(f)\n",
    "            original_gram_matrices.append(gram_matrix)\n",
    "\n",
    "        progress_bar = tqdm(range(self.num_steps), desc=\"Optimizing\", unit=\"step\")\n",
    "        if self._optim == \"LBFGS\":\n",
    "            optimizer = optim.LBFGS([random_image], lr=self.lr)\n",
    "            for step in progress_bar:\n",
    "                current_loss = [0.0]\n",
    "\n",
    "                def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    style_loss = torch.zeros([], device=self.device, requires_grad=True)\n",
    "\n",
    "                    style_outputs = self.model(random_image, self.style_layers_list)\n",
    "                    for idx, o in enumerate(style_outputs):\n",
    "                        G = self.get_gram_matrix(o)\n",
    "                        style_loss = torch.add(\n",
    "                            style_loss,\n",
    "                            torch.mean((G - original_gram_matrices[idx]) ** 2)\n",
    "                            * self.style_weights[idx],\n",
    "                        )\n",
    "\n",
    "                    style_loss.backward()\n",
    "\n",
    "                    current_loss[0] = style_loss.item()\n",
    "                    return style_loss\n",
    "\n",
    "                optimizer.step(closure)\n",
    "                progress_bar.set_postfix(loss=current_loss[0])\n",
    "\n",
    "        elif self._optim == \"Adam\":\n",
    "            optimizer = optim.Adam([random_image], lr=self.lr)\n",
    "            for step in progress_bar:\n",
    "                optimizer.zero_grad()\n",
    "                style_loss = torch.zeros([], device=self.device, requires_grad=True)\n",
    "                style_outputs = self.model(random_image, self.style_layers_list)\n",
    "\n",
    "                for idx, o in enumerate(style_outputs):\n",
    "                    G = self.get_gram_matrix(o)\n",
    "                    style_loss = torch.add(\n",
    "                        self.style_weights[idx]\n",
    "                        * torch.mean((G - original_gram_matrices[idx]) ** 2),\n",
    "                        style_loss,\n",
    "                    )\n",
    "\n",
    "                style_loss.backward()\n",
    "                optimizer.step()\n",
    "                progress_bar.set_postfix(loss=style_loss.item())\n",
    "        generated_image = self.tensor_to_image(random_image)\n",
    "        # plt.imshow(generated_image)\n",
    "        # plt.axis(\"off\")\n",
    "        # plt.show()\n",
    "        return generated_image\n",
    "\n",
    "    def style_transfer(self):\n",
    "        print(f\"Using Device: {self.device}\")\n",
    "        random_image = self.generate_white_noise(self.content_image, self.device)\n",
    "\n",
    "        original_content_features = self.model(self.content_image, self.content_layer)[\n",
    "            0\n",
    "        ]\n",
    "\n",
    "        origianl_style_features = self.model(self.style_image, self.style_layers_list)\n",
    "\n",
    "        original_gram_matrices = []\n",
    "        for style_feat in origianl_style_features:\n",
    "            gram_matrix = self.get_gram_matrix(style_feat)\n",
    "            original_gram_matrices.append(gram_matrix)\n",
    "\n",
    "        progress_bar = tqdm(range(self.num_steps), desc=\"Optimizing\", unit=\"step\")\n",
    "        if self._optim == \"LBFGS\":\n",
    "            optimizer = optim.LBFGS([random_image], lr=self.lr)\n",
    "            for step in progress_bar:\n",
    "                current_loss = [0.0]\n",
    "\n",
    "                def closure():\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    random_img_features = self.model(\n",
    "                        random_image, self.content_layer + self.style_layers_list\n",
    "                    )\n",
    "\n",
    "                    random_img_content_feat = random_img_features[0]\n",
    "                    random_img_style_feat = random_img_features[1:]\n",
    "\n",
    "                    content_loss = torch.mean(\n",
    "                        (original_content_features - random_img_content_feat) ** 2\n",
    "                    )\n",
    "\n",
    "                    style_loss = torch.tensor(\n",
    "                        0.0, device=self.device, requires_grad=True\n",
    "                    )\n",
    "                    for idx, feat in enumerate(random_img_style_feat):\n",
    "                        G = self.get_gram_matrix(feat)\n",
    "                        style_loss = torch.add(\n",
    "                            style_loss,\n",
    "                            torch.mean((G - original_gram_matrices[idx]) ** 2)\n",
    "                            * self.style_weights[idx],\n",
    "                        )\n",
    "\n",
    "                    total_loss = self.alpha * content_loss + self.beta * style_loss\n",
    "                    total_loss.backward()\n",
    "\n",
    "                    current_loss[0] = total_loss.item()\n",
    "                    return total_loss\n",
    "\n",
    "                optimizer.step(closure)\n",
    "                progress_bar.set_postfix(loss=current_loss[0])\n",
    "\n",
    "        elif self._optim == \"Adam\":\n",
    "            optimizer = optim.Adam([random_image], lr=self.lr)\n",
    "            for step in progress_bar:\n",
    "                optimizer.zero_grad()\n",
    "                random_img_features = self.model(\n",
    "                    random_image, self.content_layer + self.style_layers_list\n",
    "                )\n",
    "\n",
    "                random_img_content_feat = random_img_features[0]\n",
    "                random_img_style_feat = random_img_features[1:]\n",
    "\n",
    "                content_loss = torch.mean(\n",
    "                    (original_content_features - random_img_content_feat) ** 2\n",
    "                )\n",
    "\n",
    "                style_loss = torch.zeros([], device=self.device)\n",
    "                for idx, feat in enumerate(random_img_style_feat):\n",
    "                    G = self.get_gram_matrix(feat)\n",
    "                    style_loss += (\n",
    "                        torch.mean((G - original_gram_matrices[idx]) ** 2)\n",
    "                        * self.style_weights[idx]\n",
    "                    )\n",
    "\n",
    "                total_loss = self.alpha * content_loss + self.beta * style_loss\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                progress_bar.set_postfix(loss=total_loss.item())\n",
    "\n",
    "        generated_image = self.tensor_to_image(random_image)\n",
    "        plt.imshow(generated_image)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        return generated_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing representations from different layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(images_list, image_names):\n",
    "    for image, name in zip(images_list, image_names):\n",
    "        image.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_contents(layers=[\"1\", \"6\", \"11\", \"20\", \"29\"]):\n",
    "    generated_images = []\n",
    "    for layer_i in tqdm(layers, total=len(layers)):\n",
    "        nst = NeuralStyleTransfer(\n",
    "            content_image_path=r\"E:\\Resources\\My Projects\\A Neural Algorithm of Artistic Style - Paper Implementation\\Neural-Style-Transfer-and-Fast-Neural-Style-Transfer\\assets\\Content Images\\Tuebingen_Neckarfront.jpg\",\n",
    "            style_image_path=r\"E:\\Resources\\My Projects\\A Neural Algorithm of Artistic Style - Paper Implementation\\Neural-Style-Transfer-and-Fast-Neural-Style-Transfer\\assets\\Style Images\\the-starry-night.jpg\",\n",
    "            content_layer_name=[layer_i],  #\n",
    "            style_layers_list=[str(i) for i in [1, 6, 11, 20, 29]],  #  [1,6,11,20,29]\n",
    "            style_layer_weights=[\n",
    "                1e3 / n**2\n",
    "                for n in [\n",
    "                    64,\n",
    "                    128,\n",
    "                    256,\n",
    "                    512,\n",
    "                    512,\n",
    "                ]\n",
    "            ],\n",
    "            optimizer=\"LBFGS\",\n",
    "            num_steps=75,\n",
    "            lr=1,\n",
    "            resize_value=256,\n",
    "            alpha=1,\n",
    "            beta=1000,\n",
    "        )\n",
    "        generated_image = nst.visualize_content_rep()\n",
    "        generated_images.append(generated_image)\n",
    "    return generated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing: 100%|██████████| 75/75 [00:01<00:00, 44.34step/s, loss=1.73e-5]\n",
      " 20%|██        | 1/5 [00:03<00:13,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing: 100%|██████████| 75/75 [01:25<00:00,  1.14s/step, loss=0.00132]\n",
      " 40%|████      | 2/5 [01:29<02:36, 52.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing: 100%|██████████| 75/75 [01:41<00:00,  1.35s/step, loss=0.000431]\n",
      " 60%|██████    | 3/5 [03:12<02:30, 75.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing: 100%|██████████| 75/75 [02:30<00:00,  2.01s/step, loss=0.00111]\n",
      " 80%|████████  | 4/5 [05:45<01:45, 105.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing: 100%|██████████| 75/75 [01:18<00:00,  1.05s/step, loss=0.000155]\n",
      "100%|██████████| 5/5 [07:07<00:00, 85.51s/it] \n"
     ]
    }
   ],
   "source": [
    "generated_images = visualize_contents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"./Content_Visualization/relu1_1.png\",\n",
    "    \"./Content_Visualization/relu2_1.png\",\n",
    "    \"./Content_Visualization/relu3_1.png\",\n",
    "    \"./Content_Visualization/relu4_1.png\",\n",
    "    \"./Content_Visualization/relu5_1.png\",\n",
    "]\n",
    "\n",
    "save_images(generated_images, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
